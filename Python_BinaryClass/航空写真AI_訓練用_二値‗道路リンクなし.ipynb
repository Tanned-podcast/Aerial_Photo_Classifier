{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c95806-e9db-4a5c-b760-63006aa974c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20231219_1038\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "dt_now = datetime.datetime.now()\n",
    "print(dt_now.strftime('%Y%m%d_%H%M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15549511-7f9a-444c-a97f-3891f5f1c780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae625a8-5e02-4942-849f-3c2bff6314ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "print(np.array(torch.tensor([1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "254f9a78-ef43-430c-8b9a-e725b900efc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [8, 2]                    --\n",
       "├─Conv2d: 1-1                            [8, 64, 8, 8]             9,408\n",
       "├─BatchNorm2d: 1-2                       [8, 64, 8, 8]             128\n",
       "├─ReLU: 1-3                              [8, 64, 8, 8]             --\n",
       "├─MaxPool2d: 1-4                         [8, 64, 4, 4]             --\n",
       "├─Sequential: 1-5                        [8, 256, 4, 4]            --\n",
       "│    └─Bottleneck: 2-1                   [8, 256, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-1                  [8, 64, 4, 4]             4,096\n",
       "│    │    └─BatchNorm2d: 3-2             [8, 64, 4, 4]             128\n",
       "│    │    └─ReLU: 3-3                    [8, 64, 4, 4]             --\n",
       "│    │    └─Conv2d: 3-4                  [8, 64, 4, 4]             36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [8, 64, 4, 4]             128\n",
       "│    │    └─ReLU: 3-6                    [8, 64, 4, 4]             --\n",
       "│    │    └─Conv2d: 3-7                  [8, 256, 4, 4]            16,384\n",
       "│    │    └─BatchNorm2d: 3-8             [8, 256, 4, 4]            512\n",
       "│    │    └─Sequential: 3-9              [8, 256, 4, 4]            16,896\n",
       "│    │    └─ReLU: 3-10                   [8, 256, 4, 4]            --\n",
       "│    └─Bottleneck: 2-2                   [8, 256, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-11                 [8, 64, 4, 4]             16,384\n",
       "│    │    └─BatchNorm2d: 3-12            [8, 64, 4, 4]             128\n",
       "│    │    └─ReLU: 3-13                   [8, 64, 4, 4]             --\n",
       "│    │    └─Conv2d: 3-14                 [8, 64, 4, 4]             36,864\n",
       "│    │    └─BatchNorm2d: 3-15            [8, 64, 4, 4]             128\n",
       "│    │    └─ReLU: 3-16                   [8, 64, 4, 4]             --\n",
       "│    │    └─Conv2d: 3-17                 [8, 256, 4, 4]            16,384\n",
       "│    │    └─BatchNorm2d: 3-18            [8, 256, 4, 4]            512\n",
       "│    │    └─ReLU: 3-19                   [8, 256, 4, 4]            --\n",
       "│    └─Bottleneck: 2-3                   [8, 256, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-20                 [8, 64, 4, 4]             16,384\n",
       "│    │    └─BatchNorm2d: 3-21            [8, 64, 4, 4]             128\n",
       "│    │    └─ReLU: 3-22                   [8, 64, 4, 4]             --\n",
       "│    │    └─Conv2d: 3-23                 [8, 64, 4, 4]             36,864\n",
       "│    │    └─BatchNorm2d: 3-24            [8, 64, 4, 4]             128\n",
       "│    │    └─ReLU: 3-25                   [8, 64, 4, 4]             --\n",
       "│    │    └─Conv2d: 3-26                 [8, 256, 4, 4]            16,384\n",
       "│    │    └─BatchNorm2d: 3-27            [8, 256, 4, 4]            512\n",
       "│    │    └─ReLU: 3-28                   [8, 256, 4, 4]            --\n",
       "├─Sequential: 1-6                        [8, 512, 2, 2]            --\n",
       "│    └─Bottleneck: 2-4                   [8, 512, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-29                 [8, 128, 4, 4]            32,768\n",
       "│    │    └─BatchNorm2d: 3-30            [8, 128, 4, 4]            256\n",
       "│    │    └─ReLU: 3-31                   [8, 128, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-32                 [8, 128, 2, 2]            147,456\n",
       "│    │    └─BatchNorm2d: 3-33            [8, 128, 2, 2]            256\n",
       "│    │    └─ReLU: 3-34                   [8, 128, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-35                 [8, 512, 2, 2]            65,536\n",
       "│    │    └─BatchNorm2d: 3-36            [8, 512, 2, 2]            1,024\n",
       "│    │    └─Sequential: 3-37             [8, 512, 2, 2]            132,096\n",
       "│    │    └─ReLU: 3-38                   [8, 512, 2, 2]            --\n",
       "│    └─Bottleneck: 2-5                   [8, 512, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-39                 [8, 128, 2, 2]            65,536\n",
       "│    │    └─BatchNorm2d: 3-40            [8, 128, 2, 2]            256\n",
       "│    │    └─ReLU: 3-41                   [8, 128, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-42                 [8, 128, 2, 2]            147,456\n",
       "│    │    └─BatchNorm2d: 3-43            [8, 128, 2, 2]            256\n",
       "│    │    └─ReLU: 3-44                   [8, 128, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-45                 [8, 512, 2, 2]            65,536\n",
       "│    │    └─BatchNorm2d: 3-46            [8, 512, 2, 2]            1,024\n",
       "│    │    └─ReLU: 3-47                   [8, 512, 2, 2]            --\n",
       "│    └─Bottleneck: 2-6                   [8, 512, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-48                 [8, 128, 2, 2]            65,536\n",
       "│    │    └─BatchNorm2d: 3-49            [8, 128, 2, 2]            256\n",
       "│    │    └─ReLU: 3-50                   [8, 128, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-51                 [8, 128, 2, 2]            147,456\n",
       "│    │    └─BatchNorm2d: 3-52            [8, 128, 2, 2]            256\n",
       "│    │    └─ReLU: 3-53                   [8, 128, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-54                 [8, 512, 2, 2]            65,536\n",
       "│    │    └─BatchNorm2d: 3-55            [8, 512, 2, 2]            1,024\n",
       "│    │    └─ReLU: 3-56                   [8, 512, 2, 2]            --\n",
       "│    └─Bottleneck: 2-7                   [8, 512, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-57                 [8, 128, 2, 2]            65,536\n",
       "│    │    └─BatchNorm2d: 3-58            [8, 128, 2, 2]            256\n",
       "│    │    └─ReLU: 3-59                   [8, 128, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-60                 [8, 128, 2, 2]            147,456\n",
       "│    │    └─BatchNorm2d: 3-61            [8, 128, 2, 2]            256\n",
       "│    │    └─ReLU: 3-62                   [8, 128, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-63                 [8, 512, 2, 2]            65,536\n",
       "│    │    └─BatchNorm2d: 3-64            [8, 512, 2, 2]            1,024\n",
       "│    │    └─ReLU: 3-65                   [8, 512, 2, 2]            --\n",
       "├─Sequential: 1-7                        [8, 1024, 1, 1]           --\n",
       "│    └─Bottleneck: 2-8                   [8, 1024, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-66                 [8, 256, 2, 2]            131,072\n",
       "│    │    └─BatchNorm2d: 3-67            [8, 256, 2, 2]            512\n",
       "│    │    └─ReLU: 3-68                   [8, 256, 2, 2]            --\n",
       "│    │    └─Conv2d: 3-69                 [8, 256, 1, 1]            589,824\n",
       "│    │    └─BatchNorm2d: 3-70            [8, 256, 1, 1]            512\n",
       "│    │    └─ReLU: 3-71                   [8, 256, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-72                 [8, 1024, 1, 1]           262,144\n",
       "│    │    └─BatchNorm2d: 3-73            [8, 1024, 1, 1]           2,048\n",
       "│    │    └─Sequential: 3-74             [8, 1024, 1, 1]           526,336\n",
       "│    │    └─ReLU: 3-75                   [8, 1024, 1, 1]           --\n",
       "│    └─Bottleneck: 2-9                   [8, 1024, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-76                 [8, 256, 1, 1]            262,144\n",
       "│    │    └─BatchNorm2d: 3-77            [8, 256, 1, 1]            512\n",
       "│    │    └─ReLU: 3-78                   [8, 256, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-79                 [8, 256, 1, 1]            589,824\n",
       "│    │    └─BatchNorm2d: 3-80            [8, 256, 1, 1]            512\n",
       "│    │    └─ReLU: 3-81                   [8, 256, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-82                 [8, 1024, 1, 1]           262,144\n",
       "│    │    └─BatchNorm2d: 3-83            [8, 1024, 1, 1]           2,048\n",
       "│    │    └─ReLU: 3-84                   [8, 1024, 1, 1]           --\n",
       "│    └─Bottleneck: 2-10                  [8, 1024, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-85                 [8, 256, 1, 1]            262,144\n",
       "│    │    └─BatchNorm2d: 3-86            [8, 256, 1, 1]            512\n",
       "│    │    └─ReLU: 3-87                   [8, 256, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-88                 [8, 256, 1, 1]            589,824\n",
       "│    │    └─BatchNorm2d: 3-89            [8, 256, 1, 1]            512\n",
       "│    │    └─ReLU: 3-90                   [8, 256, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-91                 [8, 1024, 1, 1]           262,144\n",
       "│    │    └─BatchNorm2d: 3-92            [8, 1024, 1, 1]           2,048\n",
       "│    │    └─ReLU: 3-93                   [8, 1024, 1, 1]           --\n",
       "│    └─Bottleneck: 2-11                  [8, 1024, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-94                 [8, 256, 1, 1]            262,144\n",
       "│    │    └─BatchNorm2d: 3-95            [8, 256, 1, 1]            512\n",
       "│    │    └─ReLU: 3-96                   [8, 256, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-97                 [8, 256, 1, 1]            589,824\n",
       "│    │    └─BatchNorm2d: 3-98            [8, 256, 1, 1]            512\n",
       "│    │    └─ReLU: 3-99                   [8, 256, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-100                [8, 1024, 1, 1]           262,144\n",
       "│    │    └─BatchNorm2d: 3-101           [8, 1024, 1, 1]           2,048\n",
       "│    │    └─ReLU: 3-102                  [8, 1024, 1, 1]           --\n",
       "│    └─Bottleneck: 2-12                  [8, 1024, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-103                [8, 256, 1, 1]            262,144\n",
       "│    │    └─BatchNorm2d: 3-104           [8, 256, 1, 1]            512\n",
       "│    │    └─ReLU: 3-105                  [8, 256, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-106                [8, 256, 1, 1]            589,824\n",
       "│    │    └─BatchNorm2d: 3-107           [8, 256, 1, 1]            512\n",
       "│    │    └─ReLU: 3-108                  [8, 256, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-109                [8, 1024, 1, 1]           262,144\n",
       "│    │    └─BatchNorm2d: 3-110           [8, 1024, 1, 1]           2,048\n",
       "│    │    └─ReLU: 3-111                  [8, 1024, 1, 1]           --\n",
       "│    └─Bottleneck: 2-13                  [8, 1024, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-112                [8, 256, 1, 1]            262,144\n",
       "│    │    └─BatchNorm2d: 3-113           [8, 256, 1, 1]            512\n",
       "│    │    └─ReLU: 3-114                  [8, 256, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-115                [8, 256, 1, 1]            589,824\n",
       "│    │    └─BatchNorm2d: 3-116           [8, 256, 1, 1]            512\n",
       "│    │    └─ReLU: 3-117                  [8, 256, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-118                [8, 1024, 1, 1]           262,144\n",
       "│    │    └─BatchNorm2d: 3-119           [8, 1024, 1, 1]           2,048\n",
       "│    │    └─ReLU: 3-120                  [8, 1024, 1, 1]           --\n",
       "├─Sequential: 1-8                        [8, 2048, 1, 1]           --\n",
       "│    └─Bottleneck: 2-14                  [8, 2048, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-121                [8, 512, 1, 1]            524,288\n",
       "│    │    └─BatchNorm2d: 3-122           [8, 512, 1, 1]            1,024\n",
       "│    │    └─ReLU: 3-123                  [8, 512, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-124                [8, 512, 1, 1]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-125           [8, 512, 1, 1]            1,024\n",
       "│    │    └─ReLU: 3-126                  [8, 512, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-127                [8, 2048, 1, 1]           1,048,576\n",
       "│    │    └─BatchNorm2d: 3-128           [8, 2048, 1, 1]           4,096\n",
       "│    │    └─Sequential: 3-129            [8, 2048, 1, 1]           2,101,248\n",
       "│    │    └─ReLU: 3-130                  [8, 2048, 1, 1]           --\n",
       "│    └─Bottleneck: 2-15                  [8, 2048, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-131                [8, 512, 1, 1]            1,048,576\n",
       "│    │    └─BatchNorm2d: 3-132           [8, 512, 1, 1]            1,024\n",
       "│    │    └─ReLU: 3-133                  [8, 512, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-134                [8, 512, 1, 1]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-135           [8, 512, 1, 1]            1,024\n",
       "│    │    └─ReLU: 3-136                  [8, 512, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-137                [8, 2048, 1, 1]           1,048,576\n",
       "│    │    └─BatchNorm2d: 3-138           [8, 2048, 1, 1]           4,096\n",
       "│    │    └─ReLU: 3-139                  [8, 2048, 1, 1]           --\n",
       "│    └─Bottleneck: 2-16                  [8, 2048, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-140                [8, 512, 1, 1]            1,048,576\n",
       "│    │    └─BatchNorm2d: 3-141           [8, 512, 1, 1]            1,024\n",
       "│    │    └─ReLU: 3-142                  [8, 512, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-143                [8, 512, 1, 1]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-144           [8, 512, 1, 1]            1,024\n",
       "│    │    └─ReLU: 3-145                  [8, 512, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-146                [8, 2048, 1, 1]           1,048,576\n",
       "│    │    └─BatchNorm2d: 3-147           [8, 2048, 1, 1]           4,096\n",
       "│    │    └─ReLU: 3-148                  [8, 2048, 1, 1]           --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [8, 2048, 1, 1]           --\n",
       "├─Linear: 1-10                           [8, 2]                    4,098\n",
       "==========================================================================================\n",
       "Total params: 23,512,130\n",
       "Trainable params: 23,512,130\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 253.79\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 8.29\n",
       "Params size (MB): 94.05\n",
       "Estimated Total Size (MB): 102.36\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PyTorchはモデルの最終レイヤに損失関数とsoftmaxが入っていない loss_fnに入っているのでカスタム可能\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "resnet50 = models.resnet50(weights='DEFAULT')\n",
    "\n",
    "\n",
    "resnet50.conv1 = torch.nn.Conv2d(3,64,kernel_size = (7,7),stride = (2,2), padding = (3,3), bias = False)\n",
    "num_ftrs = resnet50.fc.in_features\n",
    "#modifying final layer\n",
    "resnet50.fc = nn.Linear(num_ftrs,2)\n",
    "summary(model=resnet50, input_size=(8, 3, 16, 16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d959f0-11e5-411d-be75-168bc630f046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE Loss: 0.6826429963111877\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# モデルの出力（予測された確率）\n",
    "output = torch.randn(1, requires_grad=True)  # 例えば、モデルの出力をランダムに生成\n",
    "\n",
    "# 正解ラベル（0または1）\n",
    "target = torch.tensor([1.0])  # 例えば、1のラベル\n",
    "\n",
    "# BCE損失を計算\n",
    "loss_fn = nn.BCELoss()\n",
    "loss = loss_fn(torch.sigmoid(output), target)\n",
    "\n",
    "print(\"BCE Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fd2ed87-515c-4d0a-aabf-cec3076bae41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0 IMGSIZE:( 3 224 224 ) BatchSize: 8 Epochs: 120 Valsplit: 0.25 Class: ['Damage', 'NoDamage'] Final Layer Nodes: 2048\n",
      "TrainData Size: 189 TrainData Batches 24 ValData Size: 63 ValData Batches: 8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 2])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 188\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel saved in\u001b[39m\u001b[38;5;124m'\u001b[39m, savepath2)\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 188\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresnet50\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m#グラフ可視化\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m#pltの(x,y)のxにあたるepochのリスト　1から開始\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ep\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 137\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(trainloader, valloader, model, loss_fn, optimizer, epochs)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n\u001b[0;32m    136\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m--> 137\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m    140\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\functional.py:3118\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3116\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3121\u001b[0m     )\n\u001b[0;32m   3123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 2])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set() # sns.set() ==> グラフの見た目をseabornに合わせる\n",
    "%matplotlib inline\n",
    "\n",
    "time = datetime.datetime.now()\n",
    "filedate=time.strftime('%Y%m%d_%H%M')\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "Channels=3\n",
    "batch_size=8\n",
    "epochs=120\n",
    "IMG_SIZE=224\n",
    "valsplit=0.25\n",
    "\n",
    "Classes = [\"Damage\", \"NoDamage\"]\n",
    "ClassNum = len(Classes)\n",
    "\n",
    "trainpath=r\"C:\\Users\\kyohe\\Aerial-Photo-Classifier\\0522Data\\Train\"\n",
    "savepath=r\"C:\\Users\\kyohe\\Aerial-Photo-Classifier\\0522Data\\Weights\"\n",
    "historypath=r\"C:\\Users\\kyohe\\Aerial-Photo-Classifier\\0522Data\\History\"\n",
    "\n",
    "'''\n",
    "PytorchではDataloaderという,膨大なデータセットからでもメモリを圧迫せずに取り出せてforループにも対応するための枠組みがある\n",
    "データセットをDataloaderが引っ張ってこれるような形式にするためにMyDataset(torch.utils.data.Dataset)というクラスを作れば，\n",
    "あとはそのメソッドをtorch.utils.data.Datasetが勝手に使用してデータを加工してくれる\n",
    "__init__, __getitem__, __len__をクラス内で必ず定義しなければならない\n",
    "Dataloader内のデータはバッチごとにまとめられる\n",
    "'''\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root: str, transforms, Classes) -> None:\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "        self.Classes = Classes\n",
    "        #globは複数のファイルのパスをまとめて取得する\n",
    "        #訓練と訓練白黒の二個下のディレクトリから画像を取得\n",
    "        self.data = list(sorted(Path(root).glob(\"*\\*\")))\n",
    "\n",
    "\n",
    "    # ここで取り出すデータを指定している\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        data = self.data[index]\n",
    "\n",
    "        img1 = cv2.imread(str(data))\n",
    "        img1 = cv2.resize(img1, (IMG_SIZE, IMG_SIZE))\n",
    "        img1 = TF.to_tensor(img1)\n",
    "\n",
    "        # データの変形 (transforms)\n",
    "        transformed_img = self.transforms(img1)\n",
    "\n",
    "        #ラベル貼り：dataというパスを/で区切ってリストにし，クラス名のところをラベルに格納\n",
    "        #クラス名は文字列なので，self.Classesの要素と比較して一致するところの番号をラベルとする\n",
    "        label = str(data).split(\"\\\\\")[-2]\n",
    "        label = torch.tensor(self.Classes.index(label))\n",
    "\n",
    "        return transformed_img, label\n",
    "\n",
    "    # この method がないと DataLoader を呼び出す際にエラーを吐かれる\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "#入力データに施す処理\n",
    "transforms = v2.Compose([\n",
    "        #v2.RandomHorizontalFlip(p=0.5),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=[0,0,0], std=[0.2, 0.2, 0.2]),\n",
    "])\n",
    "\n",
    "\n",
    "trainvalset = MyDataset(root=trainpath, transforms=transforms, Classes=Classes)\n",
    "trainset, valset = random_split(trainvalset, [1-valsplit, valsplit])\n",
    "\n",
    "trainloader = DataLoader(dataset=trainset,batch_size=batch_size,shuffle=True)\n",
    "valloader = DataLoader(dataset=valset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "resnet50 = models.resnet50(weights='DEFAULT')\n",
    "\n",
    "\n",
    "resnet50.conv1 = torch.nn.Conv2d(Channels,64,kernel_size = (7,7),stride = (2,2), padding = (3,3), bias = False)\n",
    "num_ftrs = resnet50.fc.in_features\n",
    "#modifying final layer\n",
    "resnet50.fc = nn.Linear(num_ftrs,ClassNum)\n",
    "\n",
    "#GPUにニューラルネットワークを渡す\n",
    "resnet50=resnet50.to(device)\n",
    "\n",
    "#lossfunction&optimizer\n",
    "#二値なのでBinaryCrossEntropy\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(resnet50.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "print(\"Device:\", device, \"IMGSIZE:(\", Channels, IMG_SIZE, IMG_SIZE, \")\",  \"BatchSize:\", batch_size, \"Epochs:\", epochs, \"Valsplit:\", valsplit, \"Class:\", Classes, \"Final Layer Nodes:\", num_ftrs)\n",
    "\n",
    "'''\n",
    "trainiterator\n",
    "enumerateはtrainloader内のすべてのdataに対してループし，繰り返し回数をbatchに渡すという意味\n",
    "Dataloader内ではバッチごとにデータがまとめられるので1回の取り出しで1バッチ分のデータを丸々取り出せる\n",
    "'''\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "def train(trainloader, valloader, model, loss_fn, optimizer, epochs):\n",
    "    size = len(trainloader.dataset)\n",
    "    num_batches = len(trainloader)\n",
    "    size_val = len(valloader.dataset)\n",
    "    num_batches_val = len(valloader)\n",
    "    print(\"TrainData Size:\", size, \"TrainData Batches\", num_batches, \"ValData Size:\", size_val, \"ValData Batches:\", num_batches_val)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss, running_correct = 0, 0\n",
    "        val_loss, val_correct = 0, 0\n",
    "        for batch, (X, y) in enumerate(trainloader):\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(torch.sigmoid(pred), y)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #損失と正解数の合計を計算しておき，後でそのエポック内での平均をとる\n",
    "            running_loss += loss.item()\n",
    "            running_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / num_batches\n",
    "        epoch_acc = running_correct / size\n",
    "\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "\n",
    "        print(f'Epoch {epoch + 1} TrainLoss: {epoch_loss:.4f} TrainAcc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "        model.eval()\n",
    "        # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "        # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "        with torch.no_grad():\n",
    "            for X, y in valloader:\n",
    "                X=X.to(device)\n",
    "                y=y.to(device)\n",
    "                pred = model(X)\n",
    "                val_loss += loss_fn(torch.sigmoid(pred), y).item()\n",
    "                val_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "            val_loss /= num_batches_val\n",
    "            val_correct /= size_val\n",
    "        # Set the model to train mode\n",
    "        model.train()\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_correct)\n",
    "\n",
    "        print(f'Epoch {epoch + 1} ValLoss: {val_loss:.4f} ValAcc: {val_correct:.4f}')\n",
    "\n",
    "        #10epoch毎にパラメータを別々のフォルダに保存\n",
    "        e1=epoch+1\n",
    "        if (e1 % 10 == 0):\n",
    "            savepath2 = Path(savepath+\"\\\\\"+str(e1)+\"\\model_weights\"+filedate+\".pth\")\n",
    "            print('Saving Model...')\n",
    "            torch.save(model.state_dict(), savepath2)\n",
    "            print('Model saved in', savepath2)\n",
    "\n",
    "        print(' ')\n",
    "\n",
    "train(trainloader, valloader, resnet50, loss_fn, optimizer, epochs)\n",
    "\n",
    "#グラフ可視化\n",
    "#pltの(x,y)のxにあたるepochのリスト　1から開始\n",
    "ep=np.arange(1, epochs+1)\n",
    "\n",
    "#グラフにプロットする正解率と損失のリストをCSVに書き出し to_csvは新規フォルダを作ってくれないのでosで作る\n",
    "historypath2 = str(Path(historypath+\"\\\\\"+filedate))\n",
    "os.mkdir(historypath2)\n",
    "pd.DataFrame(data=np.array([ep, train_accuracies]).T, columns=[\"Epoch\", \"Accuracy\"]).to_csv(Path(historypath2+\"\\\\\"+\"Train_Acc.csv\"))\n",
    "pd.DataFrame(data=np.array([ep, train_losses]).T, columns=[\"Epoch\", \"Loss\"]).to_csv(Path(historypath2+\"\\\\\"+\"Train_Loss.csv\"))\n",
    "pd.DataFrame(data=np.array([ep, val_accuracies]).T, columns=[\"Epoch\", \"Accuracy\"]).to_csv(Path(historypath2+\"\\\\\"+\"Val_Acc.csv\"))\n",
    "pd.DataFrame(data=np.array([ep, val_losses]).T, columns=[\"Epoch\", \"Loss\"]).to_csv(Path(historypath2+\"\\\\\"+\"Val_Loss.csv\"))\n",
    "print(\"CSVs saved in\", historypath2)\n",
    "\n",
    "def plot_acc(train_accuracies, val_accuracies):\n",
    "    plt.plot(ep, train_accuracies)\n",
    "    plt.plot(ep, val_accuracies)\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.xticks(np.arange(0, epochs+1, 10))\n",
    "    plt.yticks(np.arange(0.0, 1.05, 0.05))\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(train_losses, val_losses):\n",
    "    plt.plot(ep, train_losses)\n",
    "    plt.plot(ep, val_losses)\n",
    "    plt.title(\"model loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.xticks(np.arange(0, epochs+1, 10))\n",
    "    plt.ylim(bottom=-5, top=10)\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "plot_acc(train_accuracies, val_accuracies)\n",
    "plot_loss(train_losses, val_losses)\n",
    "\n",
    "print('Training Finished!!!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
