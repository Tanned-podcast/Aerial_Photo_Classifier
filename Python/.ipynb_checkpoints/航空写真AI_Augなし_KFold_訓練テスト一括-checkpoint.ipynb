{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b23b79f-0763-414e-9962-29799d614a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0, Image Size: (3, 224, 224), Batch Size: 8, Epochs: 120\n",
      "Classes: ['Damage', 'NoDamage'], K-Folds: 5\n",
      "\n",
      "\n",
      "==================== FOLD 1/5 ====================\n",
      "\n",
      "TrainData Size: 244, TrainData Batches: 25, ValData Size: 244, ValData Batches: 7\n",
      "Epoch 1 TrainLoss: 0.7018 TrainAcc: 0.3811\n",
      "Epoch 1 ValLoss: 0.6986 ValAcc: 0.1066\n",
      "\n",
      "Epoch 2 TrainLoss: 0.6904 TrainAcc: 0.4385\n",
      "Epoch 2 ValLoss: 0.6805 ValAcc: 0.1311\n",
      "\n",
      "Epoch 3 TrainLoss: 0.6630 TrainAcc: 0.5082\n",
      "Epoch 3 ValLoss: 0.6943 ValAcc: 0.1107\n",
      "\n",
      "Epoch 4 TrainLoss: 0.6461 TrainAcc: 0.5123\n",
      "Epoch 4 ValLoss: 0.6717 ValAcc: 0.1189\n",
      "\n",
      "Epoch 5 TrainLoss: 0.6426 TrainAcc: 0.5369\n",
      "Epoch 5 ValLoss: 0.6834 ValAcc: 0.0984\n",
      "\n",
      "Epoch 6 TrainLoss: 0.6528 TrainAcc: 0.5082\n",
      "Epoch 6 ValLoss: 0.6859 ValAcc: 0.1107\n",
      "\n",
      "Epoch 7 TrainLoss: 0.6211 TrainAcc: 0.5123\n",
      "Epoch 7 ValLoss: 0.6789 ValAcc: 0.1066\n",
      "\n",
      "Epoch 8 TrainLoss: 0.5981 TrainAcc: 0.5533\n",
      "Epoch 8 ValLoss: 0.6380 ValAcc: 0.1352\n",
      "\n",
      "Epoch 9 TrainLoss: 0.5826 TrainAcc: 0.5820\n",
      "Epoch 9 ValLoss: 0.6703 ValAcc: 0.1025\n",
      "\n",
      "Epoch 10 TrainLoss: 0.5706 TrainAcc: 0.5410\n",
      "Epoch 10 ValLoss: 0.8158 ValAcc: 0.1066\n",
      "\n",
      "Saving Model...\n",
      "Model saved in C:\\Users\\kyohe\\Aerial-Photo-Classifier\\20241023Data\\Weights\\10\\model_weights20250414_0737.pth\n",
      "Epoch 11 TrainLoss: 0.5428 TrainAcc: 0.5656\n",
      "Epoch 11 ValLoss: 0.7446 ValAcc: 0.1148\n",
      "\n",
      "Epoch 12 TrainLoss: 0.4818 TrainAcc: 0.6230\n",
      "Epoch 12 ValLoss: 0.6594 ValAcc: 0.1107\n",
      "\n",
      "Epoch 13 TrainLoss: 0.4476 TrainAcc: 0.6393\n",
      "Epoch 13 ValLoss: 0.6740 ValAcc: 0.0984\n",
      "\n",
      "Epoch 14 TrainLoss: 0.4479 TrainAcc: 0.6393\n",
      "Epoch 14 ValLoss: 0.6082 ValAcc: 0.1230\n",
      "\n",
      "Epoch 15 TrainLoss: 0.5189 TrainAcc: 0.5820\n",
      "Epoch 15 ValLoss: 0.6378 ValAcc: 0.1189\n",
      "\n",
      "Epoch 16 TrainLoss: 0.3986 TrainAcc: 0.6639\n",
      "Epoch 16 ValLoss: 0.5320 ValAcc: 0.1189\n",
      "\n",
      "Epoch 17 TrainLoss: 0.3715 TrainAcc: 0.6721\n",
      "Epoch 17 ValLoss: 0.7221 ValAcc: 0.1270\n",
      "\n",
      "Epoch 18 TrainLoss: 0.3359 TrainAcc: 0.6803\n",
      "Epoch 18 ValLoss: 0.6470 ValAcc: 0.1148\n",
      "\n",
      "Epoch 19 TrainLoss: 0.3110 TrainAcc: 0.7008\n",
      "Epoch 19 ValLoss: 1.1541 ValAcc: 0.1107\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 398\u001b[0m\n\u001b[0;32m    395\u001b[0m dataset \u001b[38;5;241m=\u001b[39m MyDataset(root\u001b[38;5;241m=\u001b[39mtrainpath, transforms\u001b[38;5;241m=\u001b[39mtransforms, Classes\u001b[38;5;241m=\u001b[39mClasses)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# k分割交差検証の実行\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m fold_losses, fold_accuracies, mean_loss, std_loss, mean_accuracy, std_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mrun_k_fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining with Cross Validation Finished!!!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    402\u001b[0m \u001b[38;5;66;03m# 最終結果を表示\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 324\u001b[0m, in \u001b[0;36mrun_k_fold_cross_validation\u001b[1;34m(dataset, k_folds, num_epochs)\u001b[0m\n\u001b[0;32m    321\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# モデルの訓練\u001b[39;00m\n\u001b[1;32m--> 324\u001b[0m model, train_losses, train_accuracies, val_losses, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# 結果の可視化\u001b[39;00m\n\u001b[0;32m    329\u001b[0m plot_results(train_losses, train_accuracies, val_losses, val_accuracies, fold\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 110\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, trainloader, valloader, loss_fn, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m    107\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m#_でreturnで帰ってくる３番目の戻り値は無視する\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[2], line 57\u001b[0m, in \u001b[0;36mMyDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     55\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index]\n\u001b[0;32m     56\u001b[0m img1 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;28mstr\u001b[39m(data)), cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 57\u001b[0m img1 \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m img1 \u001b[38;5;241m=\u001b[39m TF\u001b[38;5;241m.\u001b[39mto_tensor(img1)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# データの変形 (transforms)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "%matplotlib inline\n",
    "\n",
    "# 時間の取得とフォーマット\n",
    "time = datetime.datetime.now()\n",
    "filedate = time.strftime('%Y%m%d_%H%M')\n",
    "\n",
    "# デバイスの設定\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# 定数の設定\n",
    "Channels = 3\n",
    "batch_size = 8\n",
    "epochs = 200\n",
    "IMG_SIZE = 224\n",
    "valsplit = 0.25\n",
    "k_folds = 5  # k分割交差検証のk値\n",
    "\n",
    "# クラスの設定\n",
    "Classes = [\"Damage\", \"NoDamage\"]\n",
    "ClassNum = len(Classes)\n",
    "\n",
    "# パス設定\n",
    "trainpath = r\"C:\\Users\\kyohe\\Aerial-Photo-Classifier\\20241023Data\\Train\"\n",
    "savepath = r\"C:\\Users\\kyohe\\Aerial-Photo-Classifier\\20241023Data\\Weights\"\n",
    "historypath = r\"C:\\Users\\kyohe\\Aerial-Photo-Classifier\\20241023Data\\History\"\n",
    "resultspath = r\"C:\\Users\\kyohe\\Aerial-Photo-Classifier\\20241023Data\\Results\"\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root: str, transforms, Classes) -> None:\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "        self.Classes = Classes\n",
    "        self.data = list(sorted(Path(root).glob(\"*\\*\")))\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        data = self.data[index]\n",
    "        img1 = cv2.cvtColor(cv2.imread(str(data)), cv2.COLOR_BGR2RGB)\n",
    "        img1 = cv2.resize(img1, (IMG_SIZE, IMG_SIZE))\n",
    "        img1 = TF.to_tensor(img1)\n",
    "\n",
    "        # データの変形 (transforms)\n",
    "        transformed_img = self.transforms(img1)\n",
    "\n",
    "        # ラベル貼り\n",
    "        label = str(data).split(\"\\\\\")[-2]\n",
    "        label = torch.tensor(self.Classes.index(label))\n",
    "        \n",
    "        # ファイルパスも返す（結果の分析用）\n",
    "        return transformed_img, label, str(data)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "# 入力データに施す処理\n",
    "transforms = v2.Compose([\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0, 0, 0], std=[0.2, 0.2, 0.2]),\n",
    "])\n",
    "\n",
    "# モデルの初期化関数\n",
    "def initialize_model():\n",
    "    model = models.resnet50(weights='DEFAULT')\n",
    "    #チャンネル数いじれるようにしておく\n",
    "    model.conv1 = torch.nn.Conv2d(Channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, ClassNum)\n",
    "    return model.to(device)\n",
    "\n",
    "# 訓練関数\n",
    "def train_model(model, trainloader, valloader, loss_fn, optimizer, num_epochs):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    size = len(trainloader.dataset)\n",
    "    num_batches = len(trainloader)\n",
    "    size_val = len(valloader.dataset)\n",
    "    num_batches_val = len(valloader)\n",
    "    \n",
    "    print(f\"TrainData Size: {size}, TrainData Batches: {num_batches}, ValData Size: {size_val}, ValData Batches: {num_batches_val}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss, running_correct = 0, 0\n",
    "        val_loss, val_correct = 0, 0\n",
    "        \n",
    "        # 訓練モード\n",
    "        model.train()\n",
    "\n",
    "        #_でreturnで帰ってくる３番目の戻り値は無視する\n",
    "        for batch, (X, y, _) in enumerate(trainloader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # 予測と損失計算\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 損失と正解数の合計を計算\n",
    "            running_loss += loss.item()\n",
    "            running_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "        epoch_loss = running_loss / num_batches\n",
    "        epoch_acc = running_correct / size\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1} TrainLoss: {epoch_loss:.4f} TrainAcc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # 評価モード\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y, _ in valloader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                pred = model(X)\n",
    "                val_loss += loss_fn(pred, y).item()\n",
    "                val_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                \n",
    "            val_loss /= num_batches_val\n",
    "            val_correct /= size_val\n",
    "            \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_correct)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1} ValLoss: {val_loss:.4f} ValAcc: {val_correct:.4f}')\n",
    "        print()\n",
    "        \n",
    "        # 10 epoch毎にモデルを保存\n",
    "        e1 = epoch + 1\n",
    "        if (e1 % 10 == 0):\n",
    "            os.makedirs(os.path.join(savepath, str(e1)), exist_ok=True)\n",
    "            savepath2 = Path(savepath + \"\\\\\" + str(e1) + \"\\model_weights\" + filedate + \".pth\")\n",
    "            print('Saving Model...')\n",
    "            torch.save(model.state_dict(), savepath2)\n",
    "            print('Model saved in', savepath2)\n",
    "            \n",
    "    return model, train_losses, train_accuracies, val_losses, val_accuracies\n",
    "\n",
    "# 評価関数（混同行列と分類結果をCSVに出力）\n",
    "def evaluate_model(model, testloader, fold=None):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    file_paths = []\n",
    "    size_test = len(testloader.dataset)\n",
    "    test_loss, test_correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y, paths in testloader:\n",
    "            X = X.to(device)\n",
    "            outputs = model(X)\n",
    "            #torch.maxの一番目の戻り値は確率．二番目の戻り値は，クラス番号　なのでクラス番号だけ取り出す\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_loss += loss_fn(outputs, y).item()\n",
    "            test_correct += (outputs.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "            #appendでリストにリストを追加するとリスト内にリストがある二重状態になってしまう　extend関数で2重リストにすることなく格納\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            file_paths.extend(paths)\n",
    "\n",
    "        test_correct /= size_test\n",
    "\n",
    "    print(f'TestLoss: {test_loss:.4f} TestAcc: {test_correct:.4f}')\n",
    "    \n",
    "    # 混同行列の作成\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # 結果を出力\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # 分類レポートの出力\n",
    "    report = classification_report(y_true, y_pred, target_names=Classes)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # 分類結果をCSVに出力\n",
    "    results_df = pd.DataFrame({\n",
    "        'FilePath': file_paths,\n",
    "        'TrueLabel': [Classes[i] for i in y_true],\n",
    "        'PredictedLabel': [Classes[i] for i in y_pred],\n",
    "        'Correct': [y_true[i] == y_pred[i] for i in range(len(y_true))]\n",
    "    })\n",
    "    \n",
    "    # 結果ディレクトリがなければ作成\n",
    "    os.makedirs(resultspath, exist_ok=True)\n",
    "    \n",
    "    # foldが指定されている場合は、フォルドごとの結果ファイルを作成\n",
    "    if fold is not None:\n",
    "        csv_path = os.path.join(resultspath, f\"classification_results_fold{fold}_{filedate}.csv\")\n",
    "    else:\n",
    "        csv_path = os.path.join(resultspath, f\"classification_results_{filedate}.csv\")\n",
    "    \n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Classification results saved to: {csv_path}\")\n",
    "    \n",
    "    return cm, results_df, test_loss, test_correct\n",
    "\n",
    "# 結果の可視化関数\n",
    "def plot_results(train_losses, train_accuracies, val_losses, val_accuracies, fold=None):\n",
    "    ep = np.arange(1, len(train_losses) + 1)\n",
    "    \n",
    "    # 精度のプロット\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(ep, train_accuracies)\n",
    "    plt.plot(ep, val_accuracies)\n",
    "    plt.title(\"Model Accuracy\" + (f\" - Fold {fold}\" if fold else \"\"))\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(np.arange(0, len(train_losses) + 1, 10))\n",
    "    plt.yticks(np.arange(0.0, 1.05, 0.05))\n",
    "    plt.legend([\"Train\", \"Validation\"], loc=\"lower right\")\n",
    "    \n",
    "    # 損失のプロット\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(ep, train_losses)\n",
    "    plt.plot(ep, val_losses)\n",
    "    plt.title(\"Model Loss\" + (f\" - Fold {fold}\" if fold else \"\"))\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(np.arange(0, len(train_losses) + 1, 10))\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 結果を保存\n",
    "    history_fold_path = historypath + \"\\\\\" + filedate + (f\"_fold{fold}\" if fold else \"\")\n",
    "    os.makedirs(history_fold_path, exist_ok=True)\n",
    "    plt.savefig(os.path.join(history_fold_path, \"training_plot.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # CSVにも保存\n",
    "    pd.DataFrame(data=np.array([ep, train_accuracies]).T, columns=[\"Epoch\", \"Accuracy\"]).to_csv(os.path.join(history_fold_path, \"Train_Acc.csv\"))\n",
    "    pd.DataFrame(data=np.array([ep, train_losses]).T, columns=[\"Epoch\", \"Loss\"]).to_csv(os.path.join(history_fold_path, \"Train_Loss.csv\"))\n",
    "    pd.DataFrame(data=np.array([ep, val_accuracies]).T, columns=[\"Epoch\", \"Accuracy\"]).to_csv(os.path.join(history_fold_path, \"Val_Acc.csv\"))\n",
    "    pd.DataFrame(data=np.array([ep, val_losses]).T, columns=[\"Epoch\", \"Loss\"]).to_csv(os.path.join(history_fold_path, \"Val_Loss.csv\"))\n",
    "    print(f\"History files saved in {history_fold_path}\")\n",
    "\n",
    "# 混同行列の可視化\n",
    "def plot_confusion_matrix(cm, fold=None):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=Classes, yticklabels=Classes)\n",
    "    plt.title('Confusion Matrix' + (f\" - Fold {fold}\" if fold else \"\"))\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # 保存\n",
    "    history_fold_path = historypath + \"\\\\\" + filedate + (f\"_fold{fold}\" if fold else \"\")\n",
    "    os.makedirs(history_fold_path, exist_ok=True)\n",
    "    plt.savefig(os.path.join(history_fold_path, \"confusion_matrix.png\"))\n",
    "    plt.show()\n",
    "\n",
    "# k分割交差検証を実行する関数\n",
    "def run_k_fold_cross_validation(dataset, k_folds, num_epochs):\n",
    "    # k-foldの初期化　shuffleは，k個に分割する直前に，各Foldに含まれるデータをランダムに選んでくれるよ（番号が連続したところで分けないよ）ってこと\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # 各フォールドの精度を保存\n",
    "    fold_accuracies = []\n",
    "    fold_losses = []\n",
    "    \n",
    "    # 全データの添字を取得\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "\n",
    "    #各フォールドごとにループ処理　その際に（訓練データに使うデータのIDリスト，検証データに使うデータのIDリスト）というリストをFoldごとにsplitで取得\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(indices)):\n",
    "        print(f\"\\n{'='*20} FOLD {fold+1}/{k_folds} {'='*20}\\n\")\n",
    "        \n",
    "        # DataLoaderの作成\n",
    "        train_subsampler = SubsetRandomSampler(train_ids)\n",
    "        test_subsampler = SubsetRandomSampler(test_ids)\n",
    "        \n",
    "        trainloader = DataLoader(\n",
    "            dataset=dataset, \n",
    "            batch_size=batch_size,\n",
    "            sampler=train_subsampler\n",
    "        )\n",
    "        \n",
    "        testloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=test_subsampler\n",
    "        )\n",
    "        \n",
    "        # モデルの初期化\n",
    "        model = initialize_model()\n",
    "        \n",
    "        # 損失関数とオプティマイザの定義\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "        \n",
    "        # モデルの訓練\n",
    "        model, train_losses, train_accuracies, val_losses, val_accuracies = train_model(\n",
    "            model, trainloader, testloader, loss_fn, optimizer, num_epochs\n",
    "        )\n",
    "        \n",
    "        # 結果の可視化\n",
    "        plot_results(train_losses, train_accuracies, val_losses, val_accuracies, fold+1)\n",
    "        \n",
    "        # テストデータでの評価\n",
    "        cm, results_df, test_loss, test_correct = evaluate_model(model, testloader, fold+1)\n",
    "        \n",
    "        # 混同行列の可視化\n",
    "        plot_confusion_matrix(cm, fold+1)\n",
    "        \n",
    "        # モデルの保存\n",
    "        os.makedirs(os.path.join(savepath, f\"fold{fold+1}\"), exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(savepath, f\"fold{fold+1}\", f\"model_weights_{filedate}.pth\"))\n",
    "        \n",
    "        # 最終精度を保存\n",
    "        fold_accuracy = test_correct\n",
    "        fold_loss = test_loss\n",
    "        fold_accuracies.append(fold_accuracy)\n",
    "        fold_losses.append(fold_loss)\n",
    "        \n",
    "        print(f\"Fold {fold+1} -Test Loss: {fold_loss:.4f} Test Accuracy: {fold_accuracy:.4f}\")\n",
    "        \n",
    "        # 全フォールドの平均精度を計算\n",
    "        mean_accuracy = sum(fold_accuracies) / k_folds\n",
    "        std_accuracy = np.std(fold_accuracies)\n",
    "        mean_loss = sum(fold_losses) / k_folds\n",
    "        std_loss = np.std(fold_losses)\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"K-Fold Cross Validation Results for k={k_folds}:\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        for fold, loss in enumerate(fold_losses):\n",
    "            print(f\"Fold {fold+1}: {loss:.4f}\")\n",
    "        \n",
    "        for fold, accuracy in enumerate(fold_accuracies):\n",
    "            print(f\"Fold {fold+1}: {accuracy:.4f}\")\n",
    "        \n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Average Loss: {mean_loss:.4f} ± {std_loss:.4f}\")\n",
    "        print(f\"Average Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # クロスバリデーション結果をCSVに保存\n",
    "        cv_results_df = pd.DataFrame({\n",
    "            'Fold': [f\"Fold {i+1}\" for i in range(k_folds)],\n",
    "            'Loss': fold_losses,\n",
    "            'Accuracy': fold_accuracies\n",
    "        })\n",
    "\n",
    "        #DataFrameで各フォールドの値を書き終えた後に，平均値も書き込む\n",
    "        cv_results_df.loc[k_folds] = ['Average Loss', mean_loss]\n",
    "        cv_results_df.loc[k_folds+1] = ['Std Dev Loss', std_loss]\n",
    "        cv_results_df.loc[k_folds+2] = ['Average Acc.', mean_accuracy]\n",
    "        cv_results_df.loc[k_folds+3] = ['Std Dev Acc.', std_accuracy]\n",
    "        \n",
    "        cv_results_path = os.path.join(resultspath, f\"cv_results_{filedate}.csv\")\n",
    "        cv_results_df.to_csv(cv_results_path, index=False)\n",
    "        print(f\"Cross validation results saved to: {cv_results_path}\")\n",
    "        \n",
    "        return fold_losses, fold_accuracies, mean_loss, std_loss, mean_accuracy, std_accuracy\n",
    "    \n",
    "# メイン実行部分\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Device: {device}, Image Size: ({Channels}, {IMG_SIZE}, {IMG_SIZE}), Batch Size: {batch_size}, Epochs: {epochs}\")\n",
    "    print(f\"Classes: {Classes}, K-Folds: {k_folds}\\n\")\n",
    "    \n",
    "    # データセットの準備\n",
    "    dataset = MyDataset(root=trainpath, transforms=transforms, Classes=Classes)\n",
    "    \n",
    "    # k分割交差検証の実行\n",
    "    fold_losses, fold_accuracies, mean_loss, std_loss, mean_accuracy, std_accuracy = run_k_fold_cross_validation(dataset, k_folds, epochs)\n",
    "    \n",
    "    print(\"\\nTraining with Cross Validation Finished!!!\")\n",
    "    \n",
    "    # 最終結果を表示\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Average Accuracy across {k_folds} folds: {mean_loss:.4f} ± {std_loss:.4f} {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "    \n",
    "    # 計算終了時間の取得とフォーマット\n",
    "    finishtime = datetime.datetime.now()\n",
    "    finishdate = finishtime.strftime('%Y%m%d_%H%M')\n",
    "    datepath=str(Path(resultspath + \"\\calc_time_\" + filedate + \".txt\"))\n",
    "    \n",
    "    # ファイルを新規作成し、日付を書き込む\n",
    "    with open(datepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(filedate)\n",
    "        f.write(finishdate)\n",
    "\n",
    "    print(\"Calculation Finished in \", finishdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693ec588-0bb4-4b2e-be0b-8ebcc69d5414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
